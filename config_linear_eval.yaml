{
  "imagenet":
    {
      # we use the hyper-params proposed in SimCLR-v2 : https://github.com/google-research/simclr#pretraining
      "framework_config":
        {
          # ---- model
          "encoder_arch": "resnet18",
          "proj_num_layers": 2,
          "proj_hidden_dim": 256,
          "proj_dim": 128,

          # ---- Losses
          # non-contrastive: ce, uPU, nnPU
          "loss": 'nnPU',
          "prior": 0.49,  # nnPU, uPU
        },
      "data_config":
        {
          # ---- settings: pu_single_data, pu_case_control, supervised, unsupervised
          # nP =  num_labeled, nU =  num_unlabeled
          "setting": "supervised",
          "num_labeled": , # gamma = 0.5: 9000, 0.2: 4000, 0.1 = 2000, 0.05 = 1000

          "num_unlabeled": ,   # only for controlled experiments - leave blank for benchmarks
          "dataset_prior": ,   # leave blank for benchmarks

          "train_batch_size": 512,
          "test_batch_size": 512,
          "num_worker": 8
        },
      "training_config":
        {
          "epochs": 100,
          "eval_freq": 1,
          "save_model_freq": 1000,

          # ----- optimization ----
          "optimizer": "Adam", # SGD/ Adam, AdamW, LARS
          "betas": [ 0.5, 0.99 ],
          "momentum": 0.9,
          "nesterov": true,
          "amsgrad": false,
          "lr0": 0.1,                               # lars: imagenet: 0.075 * \sqrt(batch_size), cifar: 1, for bs:512
          "reg": 0.00001,                            # L2 Regularizer

          # ----- LRS ------
          "lrs": 'cosine',

          "step_size": 25,                           # for step lrs
          "milestones": [ 20, 60, 120 ],            # for multistep lrs
          "gamma": 0.5,                             # for step and multistep lrs

          "T_max": 100,                            # for cosine lrs
          "eta_min": 0.00001,                       # for cosine lrs
        }
    },

  "cifar10.dog_cat":
    {
      # we use the hyper-params proposed in SimCLR-v2 : https://github.com/google-research/simclr#pretraining
      "framework_config":
        {
          "encoder_arch": "cifar-resnet18",
          "proj_num_layers": 2,
          "proj_hidden_dim": 256,
          "proj_dim": 128,
          # Losses:
          # contrastive: ssCL, sCL, puCL
          # non-contrastive: ce, uPU, nnPU
          "loss": 'ce',
          "prior": 0.5,  # some methods depend on class prior. # pi_p* =
        },
      "data_config":
        {
          # settings: pu_single_data, pu_case_control , supervised , unsupervised
          "setting": "pu_case_control",
          "num_labeled": 5000,
          "num_unlabeled": ,   # only for controlled experiments - leave blank for benchmarks
          "dataset_prior": ,    # leave blank for benchmarks

          "train_batch_size": 512,
          "test_batch_size": 1000,
          "num_worker": 8
        },
      "training_config":
        {
          "epochs": 200,
          "eval_freq": 1,
          "save_model_freq": 50,

          # ----- optimization ----
          "optimizer": "Adam", # SGD/ Adam, AdamW, LARS
          "betas": [ 0.5, 0.99 ],
          "momentum": 0.9,
          "nesterov": true,
          "amsgrad": false,
          "lr0": 0.0005,     # lars: imagenet: 0.075 * \sqrt(batch_size), cifar: 1, for bs:512
          "reg": 0,   # L2 Regularizer

          # ----- LRS ------
          "lrs": ,

          "warmup": 0,                              # for linear warmup
          "step_size": 1,                           # for step lrs
          "milestones": [ 20, 60, 120 ],            # for multistep lrs
          "gamma": 1,                             # for step and multistep lrs

          "T_max": 200,                            # for cosine lrs
          "eta_min": 0.00001,                       # for cosine lrs
        }
    },

}
